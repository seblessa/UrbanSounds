{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "869bfaa48bc07b43"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-11-27T02:18:22.393492Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Input de DataSet"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c4c1db19cb01c9b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "datasets = [pd.read_csv(f'datasets/urbansounds_features_{i}.csv') for i in range(1, 11)]"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "8b44e345a051e112"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Clean the DataSet"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d210a4e9b159b459"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_mean_from_string(string):\n",
    "    cleaned_string = string.replace('\\n', '')\n",
    "    numbers = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", cleaned_string)\n",
    "    array = np.array(numbers, dtype=float)\n",
    "    mean_value = np.mean(array)\n",
    "    return mean_value"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "8c658efd7620a805"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for df in datasets:\n",
    "    for column in df.columns:\n",
    "        if column != 'Label':\n",
    "            if df[column].dtype != float and df[column].dtype != int:\n",
    "                df[column] = df[column].apply(calculate_mean_from_string)\n",
    "        else:\n",
    "            df[column] = df[column].str.split('-').str[1].astype(int)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "9f5fcf8e7522bca0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classification"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59f1fbbc0b0e99c3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Using TenserFlow"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f962185e1a1a37df"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Getting the best Hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47572d747e03a1ad"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "learning_rate = '0.1'"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "5b51091a19702862"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Combine all labels from different datasets\n",
    "all_labels = np.concatenate([df['Label'].values for df in datasets])\n",
    "\n",
    "# Define the stratified k-fold\n",
    "stratified_kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "69726a50cc7a3fe0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Hyperparameter Grid\n",
    "dropout_values = np.arange(0, 1, 0.01)\n",
    "patience_values = [3, 6]\n",
    "optimizers = ['adam', 'sgd', 'adagrad']\n",
    "regulizers_value = np.arange(0, 0.1, 0.01)\n",
    "batch_sizes = [32, 64, 128]"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "5b4c718736cf9ff5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def grid_search_best_parameters():\n",
    "    # Best results tracker\n",
    "    best_result = {\n",
    "        'dropout': None,\n",
    "        'patience': None,\n",
    "        'optimizer': None,\n",
    "        'regulaizer': None,\n",
    "        'batch_size': None,\n",
    "        'average_accuracy': 0\n",
    "    }\n",
    "    \n",
    "    total_iterations = len(dropout_values) * len(regulizers_value) * len(optimizers) * len(patience_values) * len(batch_sizes)\n",
    "    \n",
    "    with tqdm(total=total_iterations, desc=\"Grid Search Progress\") as pbar:\n",
    "        # Grid Search Loop\n",
    "        for dropout in dropout_values:\n",
    "            for regulizer in regulizers_value:\n",
    "                for optimizer in optimizers:\n",
    "                    for patience in patience_values:\n",
    "                        for batch_size in batch_sizes:\n",
    "                            cv_scores = []\n",
    "    \n",
    "                            for fold, (train_index, val_index) in enumerate(stratified_kfold.split(range(len(all_labels)), all_labels)):\n",
    "                                # Use the current fold as the validation set\n",
    "                                validation_dataset = datasets[fold]\n",
    "    \n",
    "                                # Combine the remaining datasets as the training set\n",
    "                                training_datasets = [dataset for index, dataset in enumerate(datasets) if index != fold]\n",
    "                                combined_df = pd.concat(training_datasets, ignore_index=True)\n",
    "    \n",
    "                                # Classification\n",
    "                                X_train = combined_df.drop('Label', axis=1)\n",
    "                                y_train = combined_df['Label']\n",
    "    \n",
    "                                # Oversample the features values using SMOTE\n",
    "                                smote = SMOTE(random_state=42)\n",
    "                                X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "                                # Standardize the feature values\n",
    "                                scaler = StandardScaler()\n",
    "                                X_train_scaled = scaler.fit_transform(X_resampled)\n",
    "    \n",
    "                                # Classification for validation set\n",
    "                                X_val = validation_dataset.drop('Label', axis=1)\n",
    "                                y_val = validation_dataset['Label']\n",
    "    \n",
    "                                # Oversample the features values using SMOTE for validation set\n",
    "                                X_val_resampled, y_val_resampled = smote.fit_resample(X_val, y_val)\n",
    "                                X_val_scaled = scaler.transform(X_val_resampled)\n",
    "    \n",
    "                                mean_neurons = (X_train_scaled.shape[1] + len(np.unique(y_resampled))) // 2\n",
    "                                num_input_neurons = X_train_scaled.shape[1]\n",
    "                                num_output_neurons = len(np.unique(y_resampled))\n",
    "                                neurons_hidden_layer = int(2 / 3 * num_input_neurons + 1 / 3 * num_output_neurons)\n",
    "    \n",
    "                                # Define and compile the model with hyperparameters\n",
    "                                model = tf.keras.Sequential([\n",
    "                                    tf.keras.layers.Dense(units=neurons_hidden_layer, activation='relu',\n",
    "                                                          input_shape=(X_train_scaled.shape[1],),\n",
    "                                                          kernel_regularizer=tf.keras.regularizers.l1_l2(l1=regulizer, l2=regulizer)),\n",
    "                                    tf.keras.layers.Dropout(dropout),\n",
    "                                    tf.keras.layers.Dense(units=mean_neurons, activation='relu'),\n",
    "                                    tf.keras.layers.Dropout(dropout),\n",
    "                                    tf.keras.layers.Dense(units=len(np.unique(y_resampled)), activation='softmax')\n",
    "                                ])\n",
    "                                model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer)\n",
    "    \n",
    "    \n",
    "                                early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience)\n",
    "                                # Train the model\n",
    "                                model.fit(X_train_scaled, y_resampled, validation_data=(X_val_scaled, y_val_resampled),\n",
    "                                          batch_size=batch_size, callbacks=[early_stopping])\n",
    "    \n",
    "                                # Evaluate the model on the validation set\n",
    "                                y_val_pred_probs = model.predict(X_val_scaled)\n",
    "                                y_val_pred = np.argmax(y_val_pred_probs, axis=1)\n",
    "    \n",
    "                                # Calculate and store accuracy for this fold\n",
    "                                fold_accuracy = accuracy_score(y_val_resampled, y_val_pred)\n",
    "                                cv_scores.append(fold_accuracy)\n",
    "    \n",
    "                                # Update progress bar\n",
    "                                pbar.update(1)\n",
    "    \n",
    "                            # Calculate and store the average accuracy for these hyperparameters\n",
    "                            overall_average_accuracy = np.mean(cv_scores)\n",
    "    \n",
    "                            # Check if the current set of hyperparameters is better than the best\n",
    "                            if overall_average_accuracy > best_result['average_accuracy']:\n",
    "                                best_result = {\n",
    "                                    'dropout': dropout,\n",
    "                                    'patience': patience,\n",
    "                                    'optimizer': optimizer,\n",
    "                                    'regulizer': regulizer,\n",
    "                                    'batch_size': batch_size,\n",
    "                                    'average_accuracy': overall_average_accuracy\n",
    "                                }"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "9e0aba1e0c33960c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_result = grid_search_best_parameters()\n",
    "dropout = best_result['dropout']\n",
    "patience = best_result['patience']\n",
    "optimizer = best_result['optimizer']\n",
    "regulizer = best_result['regulizer']\n",
    "batch_size = best_result['batch_size']"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "1f6658a46f776267"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_result_strint = f\"Best Hyperparameters and Accuracy:\\n\" \\\n",
    "                     f\"dropout: {dropout}\\n\" \\\n",
    "                     f\"patience: {patience}\\n\" \\\n",
    "                     f\"optimizer: {optimizer}\\n\" \\\n",
    "                     f\"regulizer: {regulizer}\\n\" \\\n",
    "                     f\"batch_size: {batch_size}\\n\" \\\n",
    "                     f\"average_accuracy: {best_result['average_accuracy']}\\n\"\n",
    "\n",
    "print(best_result_strint)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "f534a076bc801afd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now lets test for the best epochs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8dc501e721946da"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def find_best_epochs():\n",
    "    epochs_range=(150, 350)\n",
    "    accuracy_threshold=0.1\n",
    "\n",
    "    cv_scores = []\n",
    "\n",
    "    for num_epochs in range(epochs_range[0], epochs_range[1] + 1):\n",
    "        print(f\"Testing with {num_epochs} epochs...\")\n",
    "\n",
    "        for fold, (train_index, val_index) in enumerate(stratified_kfold.split(range(len(all_labels)), all_labels)):\n",
    "            # Use the current fold as the validation set\n",
    "            validation_dataset = datasets[fold]\n",
    "\n",
    "            # Combine the remaining datasets as the training set\n",
    "            training_datasets = [dataset for index, dataset in enumerate(datasets) if index != fold]\n",
    "            combined_df = pd.concat(training_datasets, ignore_index=True)\n",
    "\n",
    "            # Classification\n",
    "            X_train = combined_df.drop('Label', axis=1)\n",
    "            y_train = combined_df['Label']\n",
    "\n",
    "            # Oversample the features values using SMOTE\n",
    "            smote = SMOTE(random_state=42)\n",
    "            X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "            # Standardize the feature values\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_resampled)\n",
    "\n",
    "            # Classification for validation set\n",
    "            X_val = validation_dataset.drop('Label', axis=1)\n",
    "            y_val = validation_dataset['Label']\n",
    "\n",
    "            # Oversample the features values using SMOTE for validation set\n",
    "            X_val_resampled, y_val_resampled = smote.fit_resample(X_val, y_val)\n",
    "            X_val_scaled = scaler.transform(X_val_resampled)\n",
    "\n",
    "            mean_neurons = (X_train_scaled.shape[1] + len(np.unique(y_resampled))) // 2\n",
    "            num_input_neurons = X_train_scaled.shape[1]\n",
    "            num_output_neurons = len(np.unique(y_resampled))\n",
    "            neurons_hidden_layer = int(2 / 3 * num_input_neurons + 1 / 3 * num_output_neurons)\n",
    "\n",
    "            # Define and compile the model with hyperparameters\n",
    "            model = tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(units=neurons_hidden_layer, activation='relu',\n",
    "                                      input_shape=(X_train_scaled.shape[1],),\n",
    "                                      kernel_regularizer=tf.keras.regularizers.l1_l2(l1=regulizer, l2=regulizer)),\n",
    "                tf.keras.layers.Dropout(dropout),\n",
    "                tf.keras.layers.Dense(units=mean_neurons, activation='relu'),\n",
    "                tf.keras.layers.Dropout(dropout),\n",
    "                tf.keras.layers.Dense(units=len(np.unique(y_resampled)), activation='softmax')\n",
    "            ])\n",
    "            model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "            early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience)\n",
    "\n",
    "            # Train the model\n",
    "            model.fit(X_train_scaled, y_resampled, validation_data=(X_val_scaled, y_val_resampled),\n",
    "                      batch_size=batch_size, epochs=num_epochs, callbacks=[early_stopping])\n",
    "\n",
    "            # Evaluate the model on the validation set\n",
    "            y_val_pred_probs = model.predict(X_val_scaled)\n",
    "            y_val_pred = np.argmax(y_val_pred_probs, axis=1)\n",
    "\n",
    "            # Calculate and store accuracy for this fold\n",
    "            fold_accuracy = accuracy_score(y_val_resampled, y_val_pred)\n",
    "            cv_scores.append(fold_accuracy)\n",
    "\n",
    "        # Calculate and store the average accuracy for these hyperparameters\n",
    "        overall_average_accuracy = np.mean(cv_scores)\n",
    "\n",
    "        # Check if accuracy improvement is below the threshold\n",
    "        if num_epochs > epochs_range[0] and overall_average_accuracy - cv_scores[-5] < accuracy_threshold:\n",
    "            print(f\"Stopped testing at {num_epochs} epochs.\")\n",
    "            return num_epochs\n",
    "\n",
    "    print(\"Maximum number of epochs tested. Consider increasing the range.\")\n",
    "    return epochs_range[1]\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d0abd9df3a6c876"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_epochs = find_best_epochs()\n",
    "print(f\"The best number of epochs is: {best_epochs}\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "e555f4e76f492eea"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now lets train the model with the best parameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa15d340c6130829"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cv_scores = []\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(stratified_kfold.split(range(len(all_labels)), all_labels)):\n",
    "    # Use the current fold as the validation set\n",
    "    validation_dataset = datasets[fold]\n",
    "\n",
    "    # Combine the remaining datasets as the training set\n",
    "    training_datasets = [dataset for index, dataset in enumerate(datasets) if index != fold]\n",
    "    combined_df = pd.concat(training_datasets, ignore_index=True)\n",
    "\n",
    "    # Classification\n",
    "    X_train = combined_df.drop('Label', axis=1)\n",
    "    y_train = combined_df['Label']\n",
    "\n",
    "    # Oversample the features values using SMOTE\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Standardize the feature values\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_resampled)\n",
    "\n",
    "    # Classification for validation set\n",
    "    X_val = validation_dataset.drop('Label', axis=1)\n",
    "    y_val = validation_dataset['Label']\n",
    "\n",
    "    # Oversample the features values using SMOTE for validation set\n",
    "    X_val_resampled, y_val_resampled = smote.fit_resample(X_val, y_val)\n",
    "    X_val_scaled = scaler.transform(X_val_resampled)\n",
    "\n",
    "    mean_neurons = (X_train_scaled.shape[1] + len(np.unique(y_resampled))) // 2\n",
    "    num_input_neurons = X_train_scaled.shape[1]\n",
    "    num_output_neurons = len(np.unique(y_resampled))\n",
    "    neurons_hidden_layer = int(2 / 3 * num_input_neurons + 1 / 3 * num_output_neurons)\n",
    "\n",
    "    # Define and compile the model with hyperparameters\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(units=neurons_hidden_layer, activation='relu',\n",
    "                              input_shape=(X_train_scaled.shape[1],),\n",
    "                              kernel_regularizer=tf.keras.regularizers.l1_l2(l1=regulizer, l2=regulizer)),\n",
    "        tf.keras.layers.Dropout(dropout),\n",
    "        tf.keras.layers.Dense(units=mean_neurons, activation='relu'),\n",
    "        tf.keras.layers.Dropout(dropout),\n",
    "        tf.keras.layers.Dense(units=len(np.unique(y_resampled)), activation='softmax')\n",
    "    ])\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience)\n",
    "    # Train the model\n",
    "    model.fit(X_train_scaled, y_resampled, validation_data=(X_val_scaled, y_val_resampled),\n",
    "              batch_size=batch_size,epochs=best_epochs, callbacks=[early_stopping])\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    y_val_pred_probs = model.predict(X_val_scaled)\n",
    "    y_val_pred = np.argmax(y_val_pred_probs, axis=1)\n",
    "\n",
    "    # Calculate and store accuracy for this fold\n",
    "    fold_accuracy = accuracy_score(y_val_resampled, y_val_pred)\n",
    "    cv_scores.append(fold_accuracy)\n",
    "\n",
    "# Calculate and store the average accuracy for these hyperparameters\n",
    "overall_average_accuracy = np.mean(cv_scores)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "d93a9e73baae778"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Using Scikit-Learn"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "974a14f2280efa95"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize MLP classifier\n",
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=(mean_neurons,), activation=\"relu\", max_iter=300)\n",
    "\n",
    "# Store accuracy scores for each fold\n",
    "cv_scores = []\n",
    "\n",
    "# Iterate through the datasets\n",
    "for val_index, train_index in stratified_kfold.split(datasets):\n",
    "    # Split the data into training and validation sets\n",
    "    X_train_combined = np.concatenate([datasets[i] for i in train_index])\n",
    "    y_train_combined = np.concatenate([np.zeros(len(datasets[i])) + i for i in train_index])\n",
    "\n",
    "    # Use the current dataset for validation\n",
    "    X_val, y_val = datasets[val_index[0]], np.zeros(len(datasets[val_index[0]])) + val_index[0]\n",
    "\n",
    "    # Oversample the features values using SMOTE\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_combined, y_train_combined)\n",
    "\n",
    "    # Standardize the feature values\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "    # Fit the classifier\n",
    "    mlp_classifier.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "    # Predict on the validation set\n",
    "    y_val_pred_probs = mlp_classifier.predict(X_val_scaled)\n",
    "    y_val_pred = np.argmax(y_val_pred_probs, axis=1)\n",
    "\n",
    "    # Calculate and store accuracy for this fold\n",
    "    fold_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    cv_scores.append(fold_accuracy)\n",
    "\n",
    "# Calculate and store the average accuracy for these hyperparameters\n",
    "overall_average_accuracy = np.mean(cv_scores)\n",
    "print(f\"\\nOverall Average Accuracy: {overall_average_accuracy:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "a859ec96ccf1fd34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "34159ca705beb644"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "27c54484e0905b0f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "```\n",
    "### CNN\n",
    "# Reshape data for CNN\n",
    "X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1], 1))\n",
    "X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1], 1))\n",
    "\n",
    "# Convert labels to categorical one-hot encoding\n",
    "y_train_onehot = to_categorical(y_train_encoded)\n",
    "y_test_onehot = to_categorical(y_test_encoded)\n",
    "# Define the CNN model with different activation functions for hidden layers\n",
    "activation_functions = ['tanh', 'relu', 'sigmoid']\n",
    "\n",
    "for activation1 in activation_functions:\n",
    "    for activation2 in activation_functions:\n",
    "        # Define the CNN model\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(1, X_train_scaled.shape[1], 1)))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Conv2D(64, (3, 3), activation=activation1))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Conv2D(64, (3, 3), activation=activation2))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(64, activation=activation1))\n",
    "        model.add(Dense(y_train_onehot.shape[1], activation='sigmoid'))  # Sigmoid for the output layer\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# Train the model\n",
    "model.fit(X_train_reshaped, y_train_onehot, epochs=10, validation_split=0.2)\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(X_test_reshaped, y_test_onehot)\n",
    "print(f'Test \n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7b1d35b852f08d3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "55b1ea5c60f882ac"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
